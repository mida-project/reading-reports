\section{Investigation/Research}

In this section we describe our investigation and research over the proposal readings. Investigation throughout the examination of the reading facts, novel possibilities and results. Also, addressing the research reasoned conclusions of those readings.

\subsection{Gamification \& Crowdsourcing}

A paper with the called \textit{"Dr. Detective": combining gamification techniques and crowdsourcing to create a gold standard in medical text} authored by Dumitrache et al. \cite{dumitrache2013dr} proposes a design for a gamified crowdsourcing workflow to extract annotation from medical data. The importance of this paper to our work is trivial; it will support our research questions regarding medical annotations. In a context of a general crowdsourcing platform, the authors developed a game with the purpose of engaging medical experts in solving annotation tasks. The authors have incorporated incentives such as learning features, to motivate a continuous involvement of the expert crowd. In our work, we are also interested in a technique to engage our group, the Radiologists and Medical Imaging experts. Therefore we will have more and better annotations. The game was designed to identify valuable data for training machines and interpret the data relation in the context of medical diagnosing.

Supported by this paper and the work done by Aroyo et al. \cite{aroyo2013crowd}, we can solve our main problem in gathering a ground truth from experts. By reporting their results, both authors showed that the quality of the annotations by the expert crowd is higher by allowing the game users to access each others' annotations. Therefore it will increase both quality and number of annotations. The authors are proposing to test how each of the gaming features performs individually, to find knowledge regarding the application influence on the data quality. Nevertheless, the volume of the result to adapt the best needs of the user is also important. From this work, we can explore how to integrate the gaming and task crowdsourcing workflows on our work, by using the output of the workflow or by asking the crowd to validate the output.

\clearpage

\subsection{Gathering Annotated Data}

From the above section we went deeper into gathering annotated data. That said, we analysed several research works regarding this topic. In a paper with the title \textit{CrowdTruth: Machine-Human Computation Framework for Harnessing Disagreement in Gathering Annotated Data} from Inel et al. \cite{inel2014crowdtruth}, the authors are introducing a \textit{CrowdTruth} framework for machine-human computation. The implementation of this framework is a novel approach to gathering human annotation data in a wide range of annotation tasks and on a variety of media. Despite of this paper works on text, images and videos, we will focus just on images since we are working with medical imaging, only.

The \textit{CrowdTruth} approach will be useful for us since it captures human semantics through a pipeline of three processes. The pipeline of processes are as follows on the next list.

\hfill

List of the three processes:

\hfill

\begin{enumerate}
\item Combining various machine processing of medical image in order to understand better the input content and optimise the task suitability, optimising the time and cost regarding the crowdsourcing process;
\item Providing reusable human-computing task rules to collect the maximum diversity in the human interpretation, thus collect richer human semantics;
\item Implementing \textit{CrowdTruth} metrics to support a deep analysis of the quality and semantics of the crowdsourcing data;
\end{enumerate}

\hfill

The authors are in this paper demonstrating the applicability of the approach to similar works as ours. This paper also showed us the advantages of using their open standards and their extensibility of the framework with our medical imaging multimodalities and annotation tasks.

\subsection{Interactive Learning}

%% TODO: Guerra Citations

The work done by Guerra et al. titled as \textit{Interactive Optimal Teaching with Unknown Learners} (not yet published) where the authors introduce a new approach for machine teaching, addressing a mismatch between what teacher assumes and the actual process about the learning achievements. The authors analysed several situations, by focusing their analysis on the case of a Bayesian Gaussian learning \cite{zhu2013machine} process. Therefore, this work was useful for us since their study will help us as a guide towards a compelling analysis of an interactive learning process on a User Interface (UI). The authors are presenting in this paper two sets of results. First, by involving simulation data and, second, by learning the involving task. Their results compared interactive and non-interactive methods in different scenarios.

The user study of this work could be vital to us, while the authors present how to approximate the result information with theoretical results. The authors also performed a Mann-Whitney U tests \cite{macfarland2016mann} showing us how to compare the distributions obtained with the several methods. This comparison might be relevant to our work since we are also applying and analysing several interactive processes. Also, the authors performed the Kruskal-Wallis H tests \cite{cleophas2016non} to found out the use of different values.