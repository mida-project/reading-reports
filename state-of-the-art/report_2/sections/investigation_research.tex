\section{Investigation/Research}

In this section we describe our investigation and research over the proposal readings. Investigation throughout the examination of the reading facts, novel possibilities and results. Also, addressing the research reasoned conclusions of those readings.

\subsection{Classifier Predictors}

Many learning-based algorithms are able to solve problems that previously seemed completely impossible. They can look at an image and describe what they depict in a sentence, or even turn medical diagnosis into a much more accurate by giving doctor's a second opinion. Amazing new results keep appearing every single week. However, an important thing that we need to solve is that if we deploy these Neural Networks (NN) in a production environment, we would want to know if we are relying on a good or bad decision. The narrative is very simple: if we do not trust a classifier, we will not use it. And perhaps the best way of earning the trust of a human would be if the Artificial Intelligence (AI) could explain how it came to a given decision.

Strictly speaking, a NN can explain it to us, while showing us hundreds of thousands of neuron activations that are completely unusable for any sort of intuitive reasoning. That said, what is even more difficult to solve is that this explanation happens in a way that we can interpret. An earlier approach has use decision trees that described what the learner looks at and how it uses this information to arrive to a conclusion. The work presented with the title \textit{“Why Should I Trust You?” Explaining the Predictions of Any Classifier} from Ribeiro et al. \cite{ribeiro2016should} new work is quite different. For instance, imagine that a NN would look at all the information we know about a patient and tell us that this patient likely has the flu. Also, in the meantime, it could tell us the fact that the patient has a headache and sneezes a lot contributed to the conclusion that he has the flu, but, the lack of fatigue is notable evidence against it. Our doctor could take this information and instead of blindly relying on the output, could make a more informed decision.

Other related works \cite{giger2013breast, carneiro2017automated} are describing an automated methodology for the analysis of unregistered medical imaging views in order to estimate the patient's risk of developing cancer. The main innovation behind this methodology lies in the use of Deep Learning (DP) models for the problem of jointly classifying unregistered medical imaging views and respective segmentation maps of lesions. This is a holistic methodology that can classify medical imaging exams, containing several modality of views and the segmentation maps, as opposed to the classification of individual lesions, which is the dominant approach in the field. The authors also demonstrate that the proposed system is capable of using the segmentation maps generated by automated detection systems, and still producing accurate results.

\clearpage

A big additional selling point is that these techniques are model agnostic, which means that it can be applied to other learning algorithms that are able to perform medical classification. For example, complementing medical imaging with medical text (e.g. patient's clinical history). It is also a possibility that an AI is only right by chance, and if this is the case, we should definitely know about that. And here, in this example, with the additional explanation, it is rather easy to find out that we have a bad model that looks at some modality of the medical image and acknowledge if it is a severe lesion or not.

The tests indicate that humans make significantly better decisions when they lean on explanations that are extracted by these techniques. The source code of this project is also \href{https://github.com/marcotcr/lime}{available}.